{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuAuonXp8PQv",
        "outputId": "86bdfa65-b869-4cf4-e016-c1698641a80a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.19.5)\n"
          ]
        }
      ],
      "source": [
        "pip install opencv-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFcCN6lAMUu-"
      },
      "outputs": [],
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "import cv2\n",
        "!pip install pytesseract\n",
        "import pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B045FpEST59x"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lkak_fzo9xzC",
        "outputId": "1926a812-28f5-4b85-c0c7-e30004d9049e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.7/dist-packages (0.3.8)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytesseract\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H42Q-UM5B9cC"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y Pillow\n",
        "# install the new one\n",
        "!pip install Pillow==5.3.0\n",
        "# import the new one\n",
        "import PIL\n",
        "print(PIL.PILLOW_VERSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cowXhInG_gsn",
        "outputId": "cd9e4e82-74c3-402f-d315-70234c3ae5eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement PIL (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for PIL\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "pip install PIL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0vQDEdDT7CB",
        "outputId": "459a1c0c-afa7-4523-de62-7b2250f5767d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "607IsbqSUAB7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "Path = 'drive/My Drive/Hate speech'\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv(Path+'/labeled_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7MbdibOULGb"
      },
      "outputs": [],
      "source": [
        "pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4pwvxPkUTU2",
        "outputId": "74a5af7a-1b85-4af5-e612-18752e0c3eed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import cv2\n",
        "import pytesseract\n",
        "import shutil\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import os\n",
        "import random\n",
        "\n",
        "try:\n",
        "  from PIL import Image\n",
        "\n",
        "except ImportError:\n",
        "\n",
        "  import Image\n",
        "from nltk.util import pr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import tweepy\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "import csv\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "stemmer = nltk.SnowballStemmer(\"english\")\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import streamlit as st\n",
        "import warnings\n",
        "\n",
        "\n",
        "Path = 'drive/My Drive/Hate speech'\n",
        "data = pd.read_csv(Path+'/labeled_data.csv')\n",
        "\n",
        "\n",
        "data[\"labels\"] = data[\"class\"].map({0: \"Hate Speech\", 1: \"Offensive Language\", 2: \"No Hate and Offensive\"})\n",
        "#print(data.head())\n",
        "\n",
        "data = data[[\"tweet\", \"labels\"]]\n",
        "#print(data.head())\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "stemmer = nltk.SnowballStemmer(\"english\")\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "stopword=set(stopwords.words('english'))\n",
        "def clean(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = [word for word in text.split(' ') if word not in stopword]\n",
        "    text=\" \".join(text)\n",
        "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
        "    text=\" \".join(text)\n",
        "    return text\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(clean)\n",
        "#print(data.head())\n",
        "\n",
        "x = np.array(data[\"tweet\"])\n",
        "y = np.array(data[\"labels\"])\n",
        "\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(x,y) # Fit the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train,y_train)\n",
        "clf.score(X_test,y_test)\n",
        "\n",
        "\n",
        "\n",
        "def hate_speech_detection():\n",
        "    \n",
        "    import streamlit as st\n",
        "\n",
        "    st.sidebar.title(\"Hate Content Detection\")\n",
        "    \n",
        "    select=st.sidebar.selectbox('Select Type',['None', 'Text','Image'],key=1)\n",
        "    \n",
        "    st.sidebar.title(\"Hate Content Analysis in Youtube\")\n",
        "\n",
        "    select1=st.sidebar.selectbox('Select Type' , ['None','Comments' , 'Video'],key=2)\n",
        "\n",
        "\n",
        "\n",
        "    if select ==\"None\":\n",
        "      st.write(\"\")\n",
        "\n",
        "    elif select == \"Text\":\n",
        "\n",
        "      \n",
        "      st.title(\"Hate Speech Detection in Text\")\n",
        "      user = st.text_area(\"Enter any Tweet: \")\n",
        "      Text = st.button(\"Predict Text\")\n",
        "\n",
        "      if Text:\n",
        "        \n",
        "        if len(user) < 1:\n",
        "          st.write(\"  \")\n",
        "        else:\n",
        "          sample = user\n",
        "          data = cv.transform([sample]).toarray()\n",
        "          a = clf.predict(data)\n",
        "          st.write(a)\n",
        "      \n",
        "      st.write(Text)\n",
        "\n",
        "    else:\n",
        "      st.title(\"Hate Speech Detection in Image\")\n",
        "      \n",
        "      file = st.file_uploader(\"Upload file\" , type =[\"csv\", \"png\" , \"jpg\"])\n",
        "      show_file = st.empty()\n",
        "\n",
        "      if not file:\n",
        "        show_file.info(\"Please upload a file\")\n",
        "        return\n",
        "      \n",
        "      content = file.getvalue()\n",
        "\n",
        "      if(file):\n",
        "        extractedInformation = pytesseract.image_to_string(Image.open(file))\n",
        "        sample = extractedInformation\n",
        "        data = cv.transform([sample]).toarray()\n",
        "        a = clf.predict(data)\n",
        "        st.write(a)\n",
        "      \n",
        "      else:\n",
        "        df = pd.read_csv(file)\n",
        "        st.dataframe(df.head(2))\n",
        "      \n",
        "      file.close()\n",
        "\n",
        "    if select1 == 'None':\n",
        "      st.write(\"\")\n",
        "\n",
        "    elif select1 == 'Comments':\n",
        "\n",
        "\n",
        "        st.title(\"Hate Speech Analysis in Comments\")\n",
        "\n",
        "        #api_key=\"AIzaSyA4I_AmRa5PqmMDJ5U5gGJsq8Wntf5FbaM\"\n",
        "\n",
        "        api_key = \"AIzaSyA-1me5_f4JGSZ45tmy6lHgGbwpYC8AMTo\" # Replace this dummy api key with your own.\n",
        "\n",
        "        from apiclient.discovery import build\n",
        "        youtube = build('youtube', 'v3', developerKey=api_key , cache_discovery = False)\n",
        "\n",
        "\n",
        "        ID=st.text_area(\"Enter video ID\")\n",
        "        analyze = st.button(\"Analyze\")\n",
        "\n",
        "        if(analyze):\n",
        "\n",
        "\n",
        "\n",
        "          box = [['Name', 'Comment', 'Time', 'Likes', 'Reply Count']]\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "          data = youtube.commentThreads().list(part='snippet', videoId=ID, maxResults='100', textFormat=\"plainText\").execute()\n",
        "\n",
        "          for i in data[\"items\"]:\n",
        "\n",
        "                  name = i[\"snippet\"]['topLevelComment'][\"snippet\"][\"authorDisplayName\"]\n",
        "                  comment = i[\"snippet\"]['topLevelComment'][\"snippet\"][\"textDisplay\"]\n",
        "                  published_at = i[\"snippet\"]['topLevelComment'][\"snippet\"]['publishedAt']\n",
        "                  likes = i[\"snippet\"]['topLevelComment'][\"snippet\"]['likeCount']\n",
        "                  replies = i[\"snippet\"]['totalReplyCount']\n",
        "\n",
        "                  box.append([name, comment, published_at, likes, replies])\n",
        "\n",
        "                  totalReplyCount = i[\"snippet\"]['totalReplyCount']\n",
        "\n",
        "                  if totalReplyCount > 0:\n",
        "\n",
        "                      parent = i[\"snippet\"]['topLevelComment'][\"id\"]\n",
        "\n",
        "                      data2 = youtube.comments().list(part='snippet', maxResults='100', parentId=parent, textFormat=\"plainText\").execute()\n",
        "\n",
        "                      for i in data2[\"items\"]:\n",
        "                          name = i[\"snippet\"][\"authorDisplayName\"]\n",
        "                          comment = i[\"snippet\"][\"textDisplay\"]\n",
        "                          published_at = i[\"snippet\"]['publishedAt']\n",
        "                          likes = i[\"snippet\"]['likeCount']\n",
        "                          replies = \"\"\n",
        "\n",
        "                          box.append([name, comment, published_at, likes, replies])\n",
        "\n",
        "                  while (\"nextPageToken\" in data):\n",
        "\n",
        "                    data = youtube.commentThreads().list(part='snippet', videoId=ID, pageToken=data[\"nextPageToken\"],\n",
        "                                                      maxResults='100', textFormat=\"plainText\").execute()\n",
        "\n",
        "                  for i in data[\"items\"]:\n",
        "                      name = i[\"snippet\"]['topLevelComment'][\"snippet\"][\"authorDisplayName\"]\n",
        "                      comment = i[\"snippet\"]['topLevelComment'][\"snippet\"][\"textDisplay\"]\n",
        "                      published_at = i[\"snippet\"]['topLevelComment'][\"snippet\"]['publishedAt']\n",
        "                      likes = i[\"snippet\"]['topLevelComment'][\"snippet\"]['likeCount']\n",
        "                      replies = i[\"snippet\"]['totalReplyCount']\n",
        "\n",
        "                      box.append([name, comment, published_at, likes, replies])\n",
        "\n",
        "                      totalReplyCount = i[\"snippet\"]['totalReplyCount']\n",
        "\n",
        "                      if totalReplyCount > 0:\n",
        "\n",
        "                          parent = i[\"snippet\"]['topLevelComment'][\"id\"]\n",
        "\n",
        "                          data2 = youtube.comments().list(part='snippet', maxResults='100', parentId=parent, textFormat=\"plainText\").execute()\n",
        "\n",
        "                          for i in data2[\"items\"]:\n",
        "                              name = i[\"snippet\"][\"authorDisplayName\"]\n",
        "                              comment = i[\"snippet\"][\"textDisplay\"]\n",
        "                              published_at = i[\"snippet\"]['publishedAt']\n",
        "                              likes = i[\"snippet\"]['likeCount']\n",
        "                              replies = ''\n",
        "\n",
        "                              box.append([name, comment, published_at, likes, replies])\n",
        "\n",
        "              \n",
        "            \n",
        "\n",
        "          dq = pd.DataFrame({ 'Comment': [i[1] for i in box]})\n",
        "          #dq\n",
        "\n",
        "          def cleanTxt(text):\n",
        "            text = re.sub('@[A-Za-z0–9]+', '', text) #Removing @mentions\n",
        "            text = re.sub('#', '', text) # Removing '#' hash tag\n",
        "            text = re.sub('RT[\\s]+', '', text) # Removing RT\n",
        "            text = re.sub('https?:\\/\\/\\S+', '', text) # Removing hyperlink\n",
        "          \n",
        "            return text\n",
        "\n",
        "\n",
        "          # Clean the tweets\n",
        "          dq['Comment'] = dq['Comment'].apply(cleanTxt)\n",
        "\n",
        "          # Show the cleaned tweets\n",
        "          #st.write(dq)\n",
        "\n",
        "\n",
        "\n",
        "          #Create a function to get the subjectivity\n",
        "          def getSubjectivity(text):\n",
        "            return TextBlob(text).sentiment.subjectivity\n",
        "\n",
        "          # Create a function to get the polarity\n",
        "          def getPolarity(text):\n",
        "            return  TextBlob(text).sentiment.polarity\n",
        "\n",
        "\n",
        "          # Create two new columns 'Subjectivity' & 'Polarity'\n",
        "          dq['Subjectivity'] = dq['Comment'].apply(getSubjectivity)\n",
        "          dq['Polarity'] = dq['Comment'].apply(getPolarity)\n",
        "\n",
        "          # Show the new dataframe with columns 'Subjectivity' & 'Polarity'\n",
        "          #dq\n",
        "\n",
        "          # word cloud visualization\n",
        "          allWords = ' '.join([twts for twts in dq['Comment']])\n",
        "          wordCloud = WordCloud(width=500, height=300, random_state=21, max_font_size=110).generate(allWords)\n",
        "\n",
        "\n",
        "          plt.imshow(wordCloud, interpolation=\"bilinear\")\n",
        "          plt.axis('off')\n",
        "          plt.show()\n",
        "\n",
        "          st.subheader(\"Most Commonly used words in the comment section\")\n",
        "          st.pyplot()\n",
        "\n",
        "          # Create a function to compute negative (-1), neutral (0) and positive (+1) analysis\n",
        "          \n",
        "          def getAnalysis(score):\n",
        "            if score < 0:\n",
        "              return 'Negative'\n",
        "            elif score == 0:\n",
        "              return 'Neutral'\n",
        "            else:\n",
        "              return 'Positive'\n",
        "              \n",
        "          dq['Analysis'] = dq['Polarity'].apply(getAnalysis)\n",
        "          # Show the dataframe\n",
        "          st.write(dq)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          # Plotting and visualizing the counts\n",
        "          plt.title('Sentiment Analysis')\n",
        "          plt.xlabel('Sentiment')\n",
        "          plt.ylabel('Counts')\n",
        "          dq['Analysis'].value_counts().plot(kind = 'bar')\n",
        "          plt.show()\n",
        "\n",
        "          st.pyplot()\n",
        "            \n",
        "            \n",
        "hate_speech_detection()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaLeF0vIU_EA",
        "outputId": "96d62384-7e3d-4e3b-be6a-2d901d7bd070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.878s\n",
            "your url is: https://neat-bat-25.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "image_exe.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}